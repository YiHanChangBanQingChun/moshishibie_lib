# 支持向量机分类法

1. 支持向量机的基本思想
   1. 最大间隔超平面
      1. 定义：给定一个训练数据集，找到一个超平面，使得这个超平面到各个类别的最近样本点的距离最大。
   2. 最大间隔分类器
      1. 定义：给定一个训练数据集，找到一个超平面，使得这个超平面到各个类别的最近样本点的距离最大。
   3. 超平面
      1. 定义：在 \( n \) 维空间中，超平面是一个 \( n-1 \) 维的子空间。它将空间分成两个半空间。超平面可以用一个线性方程表示：$w \cdot x + b = 0$，其中 \( w \) 是法向量，决定了超平面的方向；\( x \) 是空间中的点；\( b \) 是偏置项，决定了超平面的位置。
   4. 最优分类线
      1. 分类线不仅能将两类正确分开，而且使分类的间隔最大。
      2. 分类线方程是：$w \cdot x + b = 0, \, w \in \mathbb{R}^m, \, b \in \mathbb{R}$
      3. 根据分类间隔，可以求最优超平面的参数 $w$ 和 $b$。
   5. 约束优化问题可以用拉格朗日乘子法求解。
      1. 公式：$L(w, b, \alpha) = \frac{1}{2}||w||^2 - \sum_{i=1}^N \alpha_i(y_i(w \cdot x_i + b) - 1)$
      2. 其中：$\alpha_i \geq 0, \, i = 1, 2, \ldots, N$
      3. 可以导出：$w = \sum_{i=1}^N \alpha_i y_i x_i, \, \sum_{i=1}^N \alpha_i y_i = 0$
   6. 拉氏乘子均不为0，也就是支持向量。

2. 支持向量机升维
   支持向量机（SVM）是一种用于分类和回归分析的监督学习模型。升维（也称为核技巧）是 SVM 中的一个重要概念，特别是在处理非线性可分数据时。以下是对支持向量机升维的详细介绍：

   ## 支持向量机升维的含义

   升维是指将原始的低维数据映射到一个更高维的空间中。在这个高维空间中，数据可能变得线性可分，从而可以使用线性分类器（如线性超平面）进行分类。

   ## 为什么要升维？

   在实际应用中，许多数据集在原始的低维空间中是非线性可分的，即无法通过一个简单的线性超平面将不同类别的数据点分开。通过将数据映射到更高维的空间中，数据点之间的关系可能变得更加简单，从而可以找到一个线性超平面将数据点分开。

   ## 核技巧（Kernel Trick）

   直接将数据映射到高维空间可能计算量非常大，甚至不可行。核技巧是一种有效的计算方法，它通过使用核函数在不显式计算高维映射的情况下，计算数据点在高维空间中的内积。常用的核函数包括：

   1. **线性核（Linear Kernel）**：
      $
      K(x_i, x_j) = x_i \cdot x_j
      $
      适用于线性可分的数据。

   2. **多项式核（Polynomial Kernel）**：
      $
      K(x_i, x_j) = (x_i \cdot x_j + c)^d
      $
      其中 \( c \) 是常数，\( d \) 是多项式的度。适用于具有多项式关系的数据。

   3. **高斯径向基核（Gaussian Radial Basis Function, RBF Kernel）**：
      $
      K(x_i, x_j) = \exp\left(-\frac{\|x_i - x_j\|^2}{2\sigma^2}\right)
      $
      其中 \( \sigma \) 是核宽度参数。适用于大多数情况，特别是当数据没有明显的线性关系时。

   4. **Sigmoid 核（Sigmoid Kernel）**：
      $
      K(x_i, x_j) = \tanh(\alpha x_i \cdot x_j + c)
      $
      其中 \( \alpha \) 和 \( c \) 是常数。适用于神经网络中的激活函数。

   ## 升维的优势

   1. **处理非线性数据**：通过升维，可以将非线性可分的数据转换为线性可分的数据，从而使用线性分类器进行分类。
   2. **提高分类性能**：在高维空间中，数据点之间的关系可能变得更加简单，从而提高分类器的性能。
   3. **灵活性**：通过选择不同的核函数，可以灵活地处理各种类型的数据。

   ## 示例

   假设我们有一个二维数据集，其中两个类别的数据点无法通过一条直线分开。通过使用高斯径向基核函数（RBF 核），我们可以将数据映射到一个高维空间，在这个高维空间中，数据点可以通过一个线性超平面分开。

   ## 总结

   支持向量机中的升维是通过将数据从低维空间映射到高维空间，使得数据在高维空间中变得线性可分。核技巧是一种有效的计算方法，通过使用核函数在不显式计算高维映射的情况下，计算数据点在高维空间中的内积。升维可以处理非线性数据，提高分类性能，并提供灵活性来处理各种类型的数据。

3. 优点：
   1. 针对有限样本，目标是得到现有信息的最优解，而不是得到所有样本的最优解。
   2. svm最好选择样本的区域是在支持向量附近，也就是靠边界的，而不是在整个样本空间中。

4. 多类分类问题
   1. 基于bsvm的多类分类
      1. 1-a-r分类器，
         1. 多少类就分多少次，每次分两类，最后得到多个分类器。
      2. 1-a-1分类器
         1. 一对一分类器，每次分两类，每两类之间都有一个分类器。
